# GSPO Dynamic Sampling Configuration
# This config demonstrates how to combine GSPO algorithm with DAPO's dynamic sampling mechanism

defaults:
  - /trainer/config/ppo_trainer
  - _self_

# Override algorithm configuration for GSPO with dynamic sampling
algorithm:
  _target_: verl.trainer.config.AlgoConfig
  gamma: 1.0
  lam: 1.0
  adv_estimator: grpo  # Use GRPO advantage estimator for GSPO
  norm_adv_by_std_in_grpo: true
  use_kl_in_reward: false
  kl_penalty: kl
  
  # Dynamic sampling configuration (inspired by DAPO)
  filter_groups:
    enable: true  # Enable dynamic sampling
    metric: "seq_reward"  # Options: "seq_reward", "seq_final_reward", "acc"
    max_num_gen_batches: 5  # Maximum generation attempts per training batch
  
  kl_ctrl:
    _target_: verl.trainer.config.KLControlConfig
    type: fixed
    kl_coef: 0.0
    horizon: 10000
    target_kl: 0.1

# Actor configuration for GSPO
actor_rollout_ref:
  actor:
    policy_loss:
      loss_mode: gspo  # Use GSPO loss function
    # GSPO specific clipping ratios (dual clipping)
    clip_ratio_low: 0.0003   # Lower clipping ratio as recommended in GSPO paper
    clip_ratio_high: 0.0004  # Higher clipping ratio as recommended in GSPO paper
    # Use sequence-level aggregation mode for better GSPO performance
    loss_agg_mode: "seq-mean-token-mean"
    optim:
      lr: 1e-6
      lr_warmup_steps_ratio: 0.05
      weight_decay: 0.1
    ppo_mini_batch_size: 128
    ppo_micro_batch_size_per_gpu: 8
    entropy_coeff: 0.0
    grad_clip: 1.0
    use_kl_loss: false
    kl_loss_coef: 0.0
  
  # Rollout configuration
  rollout:
    n: 16  # Number of responses per prompt (increase for better dynamic sampling)
    name: vllm
    mode: sync
    temperature: 1.0
    top_p: 1.0
    top_k: -1
    gpu_memory_utilization: 0.8
    enable_chunked_prefill: true
  
  model:
    path: Qwen/Qwen2.5-3B-Instruct
    enable_gradient_checkpointing: true
    use_remove_padding: true

# Data configuration
data:
  train_batch_size: 512  # Larger batch size for better dynamic sampling effectiveness
  max_prompt_length: 2048
  max_response_length: 8192
  prompt_key: prompt
  truncation: error
  filter_overlong_prompts: true
  shuffle: true

# Reward model configuration (using DAPO reward manager for compatibility)
reward_model:
  reward_manager: dapo
  reward_kwargs:
    overlong_buffer_cfg:
      enable: false
      len: 4096
      penalty_factor: 1.0
      log: false
    max_resp_len: 8192

# Training configuration
trainer:
  project_name: RL-GSPO-DynamicSampling
  experiment_name: gspo-dynamic-qwen25-3b
  logger: ['console', 'wandb']
  total_epochs: 10
  total_training_steps: 500
  test_freq: 10
  save_freq: 10
  val_before_train: false
  balance_batch: true
  n_gpus_per_node: 8
  nnodes: 1
  log_val_generations: 2